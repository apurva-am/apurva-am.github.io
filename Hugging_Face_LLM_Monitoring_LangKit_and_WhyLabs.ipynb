{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apurva-am/apurva-am.github.io/blob/main/Hugging_Face_LLM_Monitoring_LangKit_and_WhyLabs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV7GJkLUqpcW"
      },
      "source": [
        "# Monitoring Large Language Models (LLMs) with LangKit & Hugging Face\n",
        "\n",
        "In this example we'll show how to generate out-of-the-box text metrics for Hugging Face LLMs using LangKit and monitor them in the WhyLabs Observability Platform.\n",
        "\n",
        "LangKit can extract relevant signals from unstructured text data, such as:\n",
        "\n",
        "- [Text Quality](https://github.com/whylabs/langkit/blob/main/langkit/docs/features/quality.md)\n",
        "- [Text Relevance](https://github.com/whylabs/langkit/blob/main/langkit/docs/features/relevance.md)\n",
        "- [Security and Privacy](https://github.com/whylabs/langkit/blob/main/langkit/docs/features/security.md)\n",
        "- [Sentiment and Toxicity](https://github.com/whylabs/langkit/blob/main/langkit/docs/features/sentiment.md)\n",
        "\n",
        "For this example, we'll use the GPT2 model since it's lightweight and easy to run without a GPU, but the example can be run any of the larger Hugging Face models.\n",
        "\n",
        "![](https://github.com/whylabs/langkit/blob/main/static/img/LangKit_graphic.png?raw=true)\n",
        "\n",
        "\n",
        "## Setup\n",
        "\n",
        "To run this notebook\n",
        "- Google Account `file > save a copy in drive`\n",
        "- [Free WhyLabs Account](https://whylabs.ai/free)\n",
        "\n",
        "Other useful links:\n",
        "- LangKit [GitHub](https://github.com/whylabs/langkit)\n",
        "- whylogs [GitHub](https://github.com/whylabs/whylogs/)\n",
        "- [Slack channel](https://bit.ly/r2ai-slack) (Ask questions after the workshop here)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run code cells by pressing the play button\n",
        "# or hitting Shift+Enter when highlighted\n",
        "print(\"Hello, World!\")"
      ],
      "metadata": {
        "id": "Vzl7FtGBno-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Hugging Face Transformers, Gradio & LangKit"
      ],
      "metadata": {
        "id": "UjKQIAHDqv3q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rfQw6fBnGnC"
      },
      "outputs": [],
      "source": [
        "%pip install transformers\n",
        "%pip install gradio\n",
        "%pip install 'langkit[all]'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ‘‹ Hello, World! Take a quick look at LangKit metrics\n",
        "\n",
        "In the below code we log a few example prompt/response pairs and send metrics to WhyLabs.\n",
        "\n"
      ],
      "metadata": {
        "id": "6zKZAcVcxsfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langkit import llm_metrics # alternatively use 'light_metrics'\n",
        "import whylogs as why\n",
        "\n",
        "# Note: llm_metrics.init() downloads models so this is slow first time.\n",
        "schema = llm_metrics.init()"
      ],
      "metadata": {
        "id": "SMKZQN7Jx1Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langkit.whylogs.samples import load_chats, show_first_chat\n",
        "\n",
        "# Let's look at what's in this toy example:\n",
        "chats = load_chats()\n",
        "print(f\"There are {len(chats)} records in this toy example data, here's the first one:\")\n",
        "show_first_chat(chats)\n",
        "\n",
        "results = why.log(chats, name=\"langkit-sample-chats-all\", schema=schema)"
      ],
      "metadata": {
        "id": "NbLW9tPWx1JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profview = results.view()\n",
        "profview.to_pandas()"
      ],
      "metadata": {
        "id": "5Okh9pKlE1BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chats"
      ],
      "metadata": {
        "id": "v3fpsofv7wsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ðŸ¤— Use LangKit to monitor LLMs with any Hugging Face model\n",
        "\n",
        "Import and ititialize the Hugging Face GPT2 model + tokenizer"
      ],
      "metadata": {
        "id": "xxBB-BDsqw-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "wedbxmUaqzAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example of loading different models\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-70b-chat-hf\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-70b-chat-hf\")"
      ],
      "metadata": {
        "id": "X3F-8o5HHveX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create GPT model function\n",
        "This will take in a prompt and return a dictionary containing the model response and prompt."
      ],
      "metadata": {
        "id": "K8A5TvEGrOsC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSrG3wNdw4SR"
      },
      "outputs": [],
      "source": [
        "def gpt_model(prompt):\n",
        "\n",
        "  # Encode the prompt\n",
        "  input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "  # Generate a response\n",
        "  output = model.generate(input_ids, max_length=100, temperature=0.8,\n",
        "                          do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "  # Decode the output\n",
        "  response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  # Combine the prompt and the output into a dictionary\n",
        "  prompt_and_response = {\n",
        "      \"prompt\": prompt,\n",
        "      \"response\": response\n",
        "  }\n",
        "\n",
        "  # print(response)\n",
        "  return prompt_and_response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_and_response = gpt_model(\"Tell me a story about a cute dog\")\n",
        "print(prompt_and_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CROG5UHfzHMl",
        "outputId": "f97c8419-0c2a-4fd4-9e7a-79a9b9c7ac1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': 'Tell me a story about a cute dog', 'response': 'Tell me a story about a cute dog and she would be so happy for us to join her as she came to your house\\n\\nShe was a little too young for my liking, so I was able to take care of the dog and make her happy.\\n\\nI have to say that my parents were really lovely, they did a great job!\\n\\nI am so glad today I got to share the first picture of her in person, her beautiful big round eyes and she is very happy'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fSFY0FZ6uYiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create & Inspect Language Metrics with LangKit\n",
        "\n",
        "LangKit provides a toolkit of metrics for LLM applications, lets initialize them and create a profile of the data that can be viewed in WhyLabs for quick analysis."
      ],
      "metadata": {
        "id": "l1lP4Ybgy2go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langkit import llm_metrics # alternatively use 'light_metrics'\n",
        "import whylogs as why\n",
        "import pandas as pd\n",
        "\n",
        "# Set to show all columns in dataframe\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "# Note: llm_metrics.init() downloads models so this is slow first time.\n",
        "schema = llm_metrics.init()"
      ],
      "metadata": {
        "id": "WjwZQ0XcuwoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = why.log(prompt_and_response, schema=schema).profile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IL3lGOL0CuD",
        "outputId": "9c4b8f63-a3e0-43c6-969b-98852bfb8092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping uploading profile to WhyLabs because no name was given with name=\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also see all our values by viewing our LangKit profile in a pandas dataframe.\n",
        "\n",
        "You can use this data in real-time to make descsion about prompts and reponses, such as setting guardrails on your model."
      ],
      "metadata": {
        "id": "MAnitqOGzZq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "profview = profile.view()\n",
        "profview.to_pandas()"
      ],
      "metadata": {
        "id": "yDHASOgz1qiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Prompts"
      ],
      "metadata": {
        "id": "D8sSzZ2Z0DFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\"What is AI?\",\n",
        "           \"Tell me a joke.\",\n",
        "           \"Who won the world series in 2021?\"]"
      ],
      "metadata": {
        "id": "qXVzRN4cuafB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num, prompt in enumerate(prompts):\n",
        "\n",
        "  prompt_and_response = gpt_model(prompt)\n",
        "\n",
        "  # initial profile schema on first profile\n",
        "  # if num == 0:\n",
        "  #   profile = why.log(prompt_and_response, schema=schema).profile()\n",
        "  profile.track(prompt_and_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHnAL_G2uanJ",
        "outputId": "b806fb98-5d92-4b97-8d8f-7d86b6bfd94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping uploading profile to WhyLabs because no name was given with name=\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "profview = profile.view()\n",
        "profview.to_pandas()"
      ],
      "metadata": {
        "id": "-u5b6DISuapg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having the distribution values are important for ML monitoring\n",
        "\n",
        "![](https://raw.githubusercontent.com/whylabs/langkit/dbc11994e094a3ade6425bdc0506cecfee724f7d/static/img/sentiment-monitor.png)"
      ],
      "metadata": {
        "id": "24onjgl8qMkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ðŸ‘€ ML Monitoring for Hugging Face LLMs in WhyLabs\n",
        "\n",
        "\n",
        "To send LangKit profiles to WhyLabs we will need three pieces of information:\n",
        "\n",
        "- API token\n",
        "- Organization ID\n",
        "- Dataset ID (or model-id)\n",
        "\n",
        "Go to [https://whylabs.ai/free](https://whylabs.ai/free) and grab a free account. You can follow along with the quick start examples or skip them if you'd like to follow this example immediately.\n",
        "\n",
        "1. Create a new project and note its ID (if it's a model project, it will look like `model-xxxx`)\n",
        "2. Create an API token from the \"Access Tokens\" tab\n",
        "3. Copy your org ID from the same \"Access Tokens\" tab\n",
        "\n",
        "Replace the placeholder string values with your own OpenAI and WhyLabs API Keys below:"
      ],
      "metadata": {
        "id": "R0Gy7-9z2tBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# set authentication & project keys\n",
        "os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = 'ORGID'\n",
        "os.environ[\"WHYLABS_API_KEY\"] = 'APIKEY'\n",
        "os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = 'MODELID'"
      ],
      "metadata": {
        "id": "Udn5aaFpomlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from whylogs.api.writer.whylabs import WhyLabsWriter\n",
        "from langkit import llm_metrics # alternatively use 'light_metrics'\n",
        "import whylogs as why\n",
        "\n",
        "# Note: llm_metrics.init() downloads models so this is slow first time.\n",
        "schema = llm_metrics.init()"
      ],
      "metadata": {
        "id": "jsRrkZGT2J-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single Profile\n",
        "telemetry_agent = WhyLabsWriter()\n",
        "profile = why.log(prompt_and_response, schema=schema)\n",
        "telemetry_agent.write(profile.view())"
      ],
      "metadata": {
        "id": "aKP7jDcu2KBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e555b4fe-556d-43c8-9013-124bd227759a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping uploading profile to WhyLabs because no name was given with name=\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 'log-8uTovv0wfPXUxePW')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will write a single profile to WhyLabs\n",
        "\n",
        "Note: you may see `Skipping uploading profile to WhyLabs because no name was given with name=` ignore for now. This message won't appear if you do not use the whylabs_anonymous session first!\n",
        "\n",
        "\n",
        "Some ways to use this:\n",
        "- LLM Security\n",
        "- Prompt engineering experimetation\n",
        "- Model performance changes on a set of prompts\n",
        "- Shadow Deployment\n",
        "- Usage Drift & Monitoring  \n",
        "\n"
      ],
      "metadata": {
        "id": "yLxwVo503YG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Back Filling with WhyLabs\n",
        "\n",
        "Write seven day prompt list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PNj5vZfHMHm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_lists = [\n",
        "    [\"How can I create a new account?\", \"Great job to the team\", \"Fantastic product, had a good experience\"],\n",
        "    [\"This product made me angry, can I return it\", \"You dumb and smell bad\", \"I hated the experience, and I was over charged\"],\n",
        "    [\"This seems amazing, could you share the pricing?\", \"Incredible site, could we setup a call?\", \"Hello! Can you kindly guide me through the documentation?\"],\n",
        "    [\"This looks impressive, could you provide some information on the cost?\", \"Stunning platform, can we arrange a chat?\", \"Hello there! Could you assist me with the documentation?\"],\n",
        "    [\"This looks remarkable, could you tell me the price range?\", \"Fantastic webpage, is it possible to organize a call?\", \"Greetings! Can you help me with the relevant documents?\"],\n",
        "    [\"This is great, Ilove it, could you inform me about the charges?\", \"love the interface, can we have a teleconference?\", \"Hello! Can I take a look at the user manuals?\"],\n",
        "    [\"This seems fantastic, how much does it cost?\", \"Excellent website, can we setup a call?\", \"Hello! Could you help me find the resource documents?\"]\n",
        "]\n"
      ],
      "metadata": {
        "id": "JUAOdL3L0M33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "aC3OiGbQUD0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "telemetry_agent = WhyLabsWriter()\n",
        "all_prompts_and_responses = []  # This list will store all the prompts and responses.\n",
        "\n",
        "\n",
        "for i, day in enumerate(prompt_lists):\n",
        "  telemetry_agent = WhyLabsWriter()\n",
        "  # walking backwards. Each dataset has to map to a date to show up as a different batch in WhyLabs\n",
        "  dt = datetime.datetime.now(tz=datetime.timezone.utc) - datetime.timedelta(days=i)\n",
        "  for prompt in day:\n",
        "    prompt_and_response = gpt_model(prompt)\n",
        "    profile = why.log(prompt_and_response, schema=schema)\n",
        "\n",
        "     # Save the prompt and its response in the list.\n",
        "    all_prompts_and_responses.append(prompt_and_response)\n",
        "\n",
        "    # set the dataset timestamp for the profile\n",
        "    profile.set_dataset_timestamp(dt)\n",
        "    telemetry_agent.write(profile.view())"
      ],
      "metadata": {
        "id": "T_5RIwTVPyHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_prompts_and_responses"
      ],
      "metadata": {
        "id": "ZeEZBvtTPyJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fun with Gradio!"
      ],
      "metadata": {
        "id": "LGtg3m297QG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "774ob8KA6TNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# set authentication & project keys\n",
        "# os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = 'ORGID'\n",
        "# os.environ[\"WHYLABS_API_KEY\"] = 'APIKEY'\n",
        "os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = 'MODELID'"
      ],
      "metadata": {
        "id": "7zOiALaNDSxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_response(message, days_to_subtract):\n",
        "\n",
        "      dt = datetime.datetime.now(tz=datetime.timezone.utc) - datetime.timedelta(days=days_to_subtract)\n",
        "\n",
        "      telemetry_agent = WhyLabsWriter()\n",
        "      prompt_and_response = gpt_model(message)\n",
        "      # prompt_and_response = gpt_model(message)\n",
        "      profile = why.log(prompt_and_response, schema=schema).profile()\n",
        "\n",
        "      profile.set_dataset_timestamp(dt)\n",
        "      telemetry_agent.write(profile.view())\n",
        "\n",
        "      # Could add gaurdrail here! (Next section)\n",
        "\n",
        "      response = prompt_and_response[\"response\"]\n",
        "\n",
        "      return response\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=llm_response,\n",
        "    inputs=[\"text\", gr.inputs.Slider(minimum=0, maximum=6, step=1, default=6, label=\"Number of days to subtract from todays date (0=today, 6=week ago)\")],\n",
        "    outputs=\"text\",\n",
        "    live=False,\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "mLAkrOeg6TQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try some prompts!\n",
        "\n",
        "6. Product is great, I love this | This is great, I realled loved it\n",
        "3. I hated the product | you dumb and smell bad\n",
        "\n",
        "Compare models in the LLM dashboard\n"
      ],
      "metadata": {
        "id": "q7vh8mPHCOY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guardrails & Validation in Environment\n",
        "\n"
      ],
      "metadata": {
        "id": "_Yyi66cHMOYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import whylogs as why\n",
        "import langkit.toxicity\n",
        "import pandas as pd\n",
        "\n",
        "from langkit import llm_metrics\n",
        "\n",
        "print(\"downloading models and initialized metrics...\")\n",
        "text_metrics_schema = llm_metrics.init()\n",
        "\n",
        "# Set to show all columns in dataframe\n",
        "pd.set_option(\"display.max_columns\", None)\n"
      ],
      "metadata": {
        "id": "NYrpt9JiazPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5f1498-d975-4143-c41d-93a85ad4270e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading models and initialized metrics...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"downloading models and initialized metrics...\")\n",
        "text_metrics_schema = llm_metrics.init()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO8u8aa1h94O",
        "outputId": "d11c711e-38c3-4f70-c59b-d7fa0b89b7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading models and initialized metrics...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_not_toxic(prompt_message):\n",
        "\n",
        "    profile = why.log({\"prompt\": prompt_message}, schema=text_metrics_schema).profile().view()\n",
        "\n",
        "    profile_dict = profile.get_column('prompt.toxicity').to_summary_dict()\n",
        "    tox_max = profile_dict['distribution/max']\n",
        "\n",
        "    print(f'Toxic score: {tox_max}')\n",
        "\n",
        "    if tox_max > 0.5:\n",
        "      return False\n",
        "    else:\n",
        "      return True"
      ],
      "metadata": {
        "id": "hZjlW5ABh961"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_not_toxic(\"Do you like fruit?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JztYHz0Bh984",
        "outputId": "f14dcbaf-5632-4e9f-f2c1-5e9eac43f7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping uploading profile to WhyLabs because no name was given with name=\n",
            "Toxic score: 0.0014600753784179688\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_not_toxic(\"You dumb and smell bad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaHs4u_8iFOD",
        "outputId": "946b9e9f-ece0-43a5-8611-0d9de093338f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping uploading profile to WhyLabs because no name was given with name=\n",
            "Toxic score: 0.9606605768203735\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Do you like fruit?\"\n",
        "\n",
        "if is_not_toxic(user_prompt):\n",
        "  prompt_and_response = gpt_model(user_prompt)\n",
        "  print(prompt_and_response['response'])\n",
        "\n",
        "else:\n",
        "  print(\"As a large language model...\")"
      ],
      "metadata": {
        "id": "p8HBESyeiHtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"You dumb and smell bad\"\n",
        "\n",
        "if is_not_toxic(user_prompt):\n",
        "  prompt_and_response = gpt_model(user_prompt)\n",
        "  print(prompt_and_response['response'])\n",
        "\n",
        "else:\n",
        "  print(\"As a large language model...\")"
      ],
      "metadata": {
        "id": "p0Fn5XYriHvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd9116a-4901-4d1e-924a-23ad7f12f0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping uploading profile to WhyLabs because no name was given with name=\n",
            "Toxic score: 0.9606605768203735\n",
            "As a large language model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See another way of doing this with [LangKit validators](https://whylabs.ai/blog/posts/safeguard-monitor-large-language-model-llm-applications)"
      ],
      "metadata": {
        "id": "7Ormim0cs0N7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Metrics\n",
        "\n",
        "[BYOF: Bring Your Own Functions - Announcing UDFs in whylogs\n",
        "](https://whylabs.ai/blog/posts/announcing-user-defined-functions-in-whylogs)"
      ],
      "metadata": {
        "id": "XbCvRbi3BGf3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDDSjr54BW-C"
      },
      "source": [
        "## Use a Rolling Logger (Optional)\n",
        "A rolling logger can be used instead of the method above to write profiles to WhyLabs at pre-defined intervals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4jz_x2GAXnP"
      },
      "outputs": [],
      "source": [
        "telemetry_agent = why.logger(mode=\"rolling\", interval=5, when=\"M\",schema=schema, base_name=\"huggingface\")\n",
        "telemetry_agent.append_writer(\"whylabs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2eoEc-zAXps",
        "outputId": "42afc84a-fd78-4be4-e867-a93872faf9e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<whylogs.api.logger.result_set.ProfileResultSet at 0x7f95d6d63b80>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Log data + model outputs to WhyLabs.ai\n",
        "telemetry_agent.log(prompt_and_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PStG7ZtAAp7l"
      },
      "outputs": [],
      "source": [
        "# Close the whylogs rolling logger when the service is shut down\n",
        "telemetry_agent.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24EqoHdgrBhQ"
      },
      "source": [
        "# Resources\n",
        "\n",
        "- [Request a Workshop Certificate](https://docs.google.com/forms/d/e/1FAIpQLScKdXX59i8P0783HKTRr7MaW65B6z55jiqpVDyOiaebHqQorQ/viewform?usp=sf_link)\n",
        "- [Intro to LangKit Example](https://github.com/whylabs/langkit/blob/main/langkit/examples/Intro_to_Langkit.ipynb)\n",
        "- [LangKit + LangChain Integration](https://github.com/whylabs/langkit/blob/main/langkit/examples/Langchain_OpenAI_LLM_Monitoring_with_WhyLabs.ipynb)\n",
        "- [LangKit GitHub](https://github.com/whylabs/langkit)\n",
        "- [whylogs GitHub](https://github.com/whylabs/whylogs)\n",
        "- [WhyLabs](https://whylabs.ai/safeguard-large-language-models)\n",
        "- [Hugging Face GPT2 Model](https://huggingface.co/gpt2)\n",
        "- [Want a promo code for more features?](https://bit.ly/coupon-wlcommunity)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JKl13TOvORA5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}